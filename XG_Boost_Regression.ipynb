{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGxxoxR3eXPN",
    "outputId": "4be136f2-fa6b-47ad-bd6a-9d62713fbb0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\anaconda\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (from xgboost) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbWQmPyhfNWm",
    "outputId": "aa5ae49c-c4f5-4c6d-8d57-55c94473fe68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the XGBoost regressor model with all hyperparameters\n",
    "xg_reg = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',  # Regression with squared loss\n",
    "    colsample_bytree=0.7,          # Subsample ratio of columns when constructing each tree\n",
    "    learning_rate=0.1,             # Step size shrinkage to prevent overfitting\n",
    "    max_depth=5,                   # Maximum depth of a tree\n",
    "    alpha=10,                      # L1 regularization term on weights\n",
    "    subsample=0.8,                 # Subsample ratio of the training instance\n",
    "    gamma=0.1,                     # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    min_child_weight=5,            # Minimum sum of instance weight (hessian) needed in a child\n",
    "    n_estimators=100,              # Number of boosting rounds\n",
    "    booster='gbtree',              # Specify which booster to use: gbtree, gblinear, or dart\n",
    "    tree_method='hist',device=\"cuda\",            # Tree construction algorithm used in XGBoost\n",
    "    n_jobs=-1,                     # Number of parallel threads used to run XGBoost\n",
    "    random_state=42                # Random number seed\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qudWAyI5fcpu"
   },
   "source": [
    "# With Grid Search cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loFR4iceeiPI",
    "outputId": "28dd9012-83e1-4ba0-e14e-eb82677c5da0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 512 candidates, totalling 1024 fits\n",
      "Total time taken: 386.36 seconds\n",
      "Average time per iteration: 0.25 seconds\n",
      "Estimated remaining time per iteration during execution.\n",
      "Best parameters found: {'alpha': 1, 'booster': 'gbtree', 'colsample_bytree': 0.7, 'gamma': 0, 'lambda': 10, 'learning_rate': 0.2, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Mean Squared Error: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [07:39:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the XGBoost regressor model\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', tree_method='hist', device='cuda', n_jobs=-1, random_state=42)  # Regression with squared loss\n",
    "\n",
    "# Define a smaller hyperparameter grid\n",
    "param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],  # Subsample ratio of columns when constructing each tree\n",
    "    'learning_rate': [0.1, 0.2],  # Step size shrinkage to prevent overfitting\n",
    "    'max_depth': [3, 5],  # Maximum depth of a tree\n",
    "    'alpha': [1, 10],  # L1 regularization term on weights\n",
    "    'lambda': [1, 10],  # L2 regularization term on weights\n",
    "    'subsample': [0.8, 1.0],  # Subsample ratio of the training instance\n",
    "    'gamma': [0, 0.1],  # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'min_child_weight': [1, 5],  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'n_estimators': [100, 200],  # Number of boosting rounds\n",
    "    'booster': ['gbtree'],  # Specify which booster to use: gbtree, gblinear, or dart\n",
    "}\n",
    "\n",
    "# Calculate the total number of iterations\n",
    "total_iterations = np.prod([len(param_grid[key]) for key in param_grid.keys()]) * 3  # Number of parameter combinations * 3 (cv folds)\n",
    "\n",
    "# Setup the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error', cv=2, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Track the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model using grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the total time taken\n",
    "total_time = time.time() - start_time\n",
    "average_time_per_iteration = total_time / total_iterations\n",
    "\n",
    "# Print the time estimation information\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per iteration: {average_time_per_iteration:.2f} seconds\")\n",
    "print(f\"Estimated remaining time per iteration during execution.\")\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jF-u5zr2g-mR"
   },
   "source": [
    "# using hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 812.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\lib\\site-packages (from hyperopt) (1.21.5)\n",
      "Requirement already satisfied: cloudpickle in c:\\anaconda\\lib\\site-packages (from hyperopt) (2.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from hyperopt) (4.64.1)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\anaconda\\lib\\site-packages (from hyperopt) (2.8.4)\n",
      "Requirement already satisfied: six in c:\\anaconda\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (from hyperopt) (1.9.1)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     -------------------------------------- 200.5/200.5 kB 6.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: future in c:\\anaconda\\lib\\site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm->hyperopt) (0.4.5)\n",
      "Installing collected packages: py4j, hyperopt\n",
      "Successfully installed hyperopt-0.2.7 py4j-0.10.9.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEfJI-jzfBmB",
    "outputId": "3c73c14b-77c0-4985-b4e6-c7b1b463d15e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:06<00:37,  2.45trial/s, best loss: 0.2099090195688917]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:43:17] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:07<00:19,  4.53trial/s, best loss: 0.20709528372344008]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:43:19] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:15<01:45,  1.32s/trial, best loss: 0.20709528372344008]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:43:27] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 21/100 [00:21<03:23,  2.57s/trial, best loss: 0.1956644517966557] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:43:32] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [01:15<04:35,  4.12s/trial, best loss: 0.19527053312735057]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:44:26] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [01:33<04:16,  4.08s/trial, best loss: 0.19527053312735057]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:44:44] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [01:46<03:41,  3.75s/trial, best loss: 0.19527053312735057]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:44:57] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [01:54<01:37,  1.80s/trial, best loss: 0.19527053312735057]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:45:05] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [01:57<00:34,  1.43trial/s, best loss: 0.19527053312735057]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:45:08] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [02:09<01:40,  2.23s/trial, best loss: 0.19527053312735057]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:45:20] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [02:19<00:58,  1.51s/trial, best loss: 0.19527053312735057]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:45:30] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [03:34<01:33,  4.05s/trial, best loss: 0.19409336216737874]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:46:46] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [03:46<01:11,  3.56s/trial, best loss: 0.19409336216737874]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:46:57] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [04:05<00:59,  4.25s/trial, best loss: 0.19409336216737874]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:47:16] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [04:17<00:17,  2.16s/trial, best loss: 0.19409336216737874]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:47:28] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [04:36<00:04,  2.46s/trial, best loss: 0.19409336216737874]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:47:47] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:41<00:00,  2.82s/trial, best loss: 0.19409336216737874]\n",
      "Best parameters: {'alpha': 0, 'booster': 2, 'colsample_bytree': 0.6778712864572316, 'gamma': 0.14787863172474586, 'lambda': 0, 'learning_rate': 0.11614975739858949, 'max_depth': 2, 'min_child_weight': 1, 'n_estimators': 2, 'subsample': 0.8045746149447889}\n",
      "Total time taken: 281.52 seconds\n",
      "Mean Squared Error with best parameters: 0.19\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import time\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for hyperopt\n",
    "def objective(params):\n",
    "    # Set the parameters that are not optimized\n",
    "    params['objective'] = 'reg:squarederror'\n",
    "    params['tree_method'] = 'hist'\n",
    "    params['device'] = 'cuda'\n",
    "    params['n_jobs'] = -1\n",
    "    params['random_state'] = 42\n",
    "\n",
    "    # Create and train the model\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions and calculate the mean squared error\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Return the loss (MSE) and status\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_space = {\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 0.7),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'max_depth': hp.choice('max_depth', [3, 5, 7]),\n",
    "    'alpha': hp.choice('alpha', [1, 10, 100]),\n",
    "    'lambda': hp.choice('lambda', [1, 10, 100]),\n",
    "    'subsample': hp.uniform('subsample', 0.8, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.2),\n",
    "    'min_child_weight': hp.choice('min_child_weight', [1, 5, 10]),\n",
    "    'n_estimators': hp.choice('n_estimators', [50, 100, 200]),\n",
    "    'booster': hp.choice('booster', ['gbtree', 'gblinear', 'dart'])\n",
    "}\n",
    "\n",
    "# Run the hyperparameter optimization using Hyperopt\n",
    "trials = Trials()\n",
    "start_time = time.time()\n",
    "best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the best parameters and the total time taken\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Convert hyperopt results to the corresponding values\n",
    "best_params['max_depth'] = [3, 5, 7][best_params['max_depth']]\n",
    "best_params['alpha'] = [1, 10, 100][best_params['alpha']]\n",
    "best_params['lambda'] = [1, 10, 100][best_params['lambda']]\n",
    "best_params['min_child_weight'] = [1, 5, 10][best_params['min_child_weight']]\n",
    "best_params['n_estimators'] = [50, 100, 200][best_params['n_estimators']]\n",
    "best_params['booster'] = ['gbtree', 'gblinear', 'dart'][best_params['booster']]\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "final_model = xgb.XGBRegressor(objective='reg:squarederror', tree_method='hist', device='cuda', n_jobs=-1, random_state=42, **best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate the mean squared error on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error with best parameters: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOFiwNifhIM-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
